import torch
import os
from torch.utils.data import Dataset, DataLoader
from torch import optim
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from MCNN import MultiScaleCNN
from torchvision.datasets import DatasetFolder
import torchvision
import torchvision.transforms as trans
from PIL import Image
from utils import calculate_means_std


class CustomDataset(Dataset):
    def __init__(self, images_path: list, images_class: list, extensions=None, transform=None):
        self.image_path = images_path
        self.images_class = images_class
        # self.image_class = pd.read_csv(images_class)  # 读取标签文件为DataFrame
        # self.mean, self.std = calculate_means_std(self.image_path)
        self.transform = transform
        # self.my_trans = trans.Compose([trans.transforms.RandomResizedCrop(224),
        #                                trans.RandomHorizontalFlip(),
        #                                trans.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),
        #                                trans.ToTensor(),
        #                                trans.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        #                                ])

    def __getitem__(self, item):
        img = Image.open(self.image_path[item])
        label = self.images_class[item]
        if self.transform is not None:
            img = self.transform(img)
        return img, torch.tensor(int(label))

    def __len__(self):
        return len(self.image_path)

    @staticmethod
    def collate_fn(batch):
        images, lables = tuple(zip(*batch))
        # # 读取每个图像并将其转换为张量
        # image_tensors = []
        # for i in range(images):
        #     image_tensors.append(image)
        # 计算所有图像中的最大高度
        max_height = max([image.shape[1] for image in images])
        # 对每个图像应用填充以使它们具有相同的高度
        padded_image_tensors = []
        paddings = []
        for image in images:
            pad_height = max_height - image.shape[1]
            padding = trans.Pad((0, 0, 0, pad_height), fill=0)
            padded_image = padding(image)
            padded_image_tensors.append(padded_image)
            paddings.append(pad_height)
        images = torch.stack(tuple(padded_image_tensors), dim=0)
        lables = torch.as_tensor(lables)
        paddings = torch.as_tensor(paddings)
        return images, lables, paddings


import math
import torch
from torch.utils.data.sampler import RandomSampler


class BatchSchedulerSampler(torch.utils.data.sampler.Sampler):
    """
    iterate over tasks and provide a random batch per task in each mini-batch
    """
    def __init__(self, dataset, batch_size):
        self.dataset = dataset
        self.batch_size = batch_size
        self.number_of_datasets = len(dataset.datasets)
        self.largest_dataset_size = max([len(cur_dataset.image_path) for cur_dataset in dataset.datasets])

    def __len__(self):
        return self.batch_size * math.ceil(self.largest_dataset_size / self.batch_size) * len(self.dataset.datasets)

    def __iter__(self):
        samplers_list = []
        sampler_iterators = []
        for dataset_idx in range(self.number_of_datasets):
            cur_dataset = self.dataset.datasets[dataset_idx]
            sampler = RandomSampler(cur_dataset)  # 先对每个数据集的iterator进行shuffle
            samplers_list.append(sampler)
            cur_sampler_iterator = sampler.__iter__()
            sampler_iterators.append(cur_sampler_iterator)

        push_index_val = [0] + self.dataset.cumulative_sizes[:-1]  # 找到每个dataset第一个数据的index
        step = self.batch_size * self.number_of_datasets   # 步长为每个dataset的mini_batch_size之和
        samples_to_grab = self.batch_size  # 每个dataset的mini_batch_size
        # for this case we want to get all samples in dataset, this force us to resample from the smaller datasets
        epoch_samples = self.largest_dataset_size * self.number_of_datasets  # sample数量小的dataset会循环提取

        final_samples_list = []  # this is a list of indexes from the combined dataset
        for _ in range(0, epoch_samples, step):
            for i in range(self.number_of_datasets):
                cur_batch_sampler = sampler_iterators[i]
                cur_samples = []
                for _ in range(samples_to_grab):
                    try:
                        cur_sample_org = cur_batch_sampler.__next__()
                        cur_sample = cur_sample_org + push_index_val[i]
                        cur_samples.append(cur_sample)
                    except StopIteration:
                        # got to the end of iterator - restart the iterator and continue to get samples
                        # until reaching "epoch_samples"
                        sampler_iterators[i] = samplers_list[i].__iter__()
                        cur_batch_sampler = sampler_iterators[i]
                        cur_sample_org = cur_batch_sampler.__next__()
                        cur_sample = cur_sample_org + push_index_val[i]
                        cur_samples.append(cur_sample)
                final_samples_list.extend(cur_samples)
        return iter(final_samples_list)