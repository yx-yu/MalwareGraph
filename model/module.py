import torch
import torch.nn as nn
import hparams as hp
import numpy as np
import os
from torch.optim import Optimizer
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import Dataset, DataLoader, random_split
import math
import tqdm
import copy
from tensorboardX import SummaryWriter
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss







def plot_embedding(data, label, title):
	x_min, x_max = np.min(data, 0), np.max(data, 0)
	data = (data - x_min) / (x_max - x_min)
	fig = plt.figure()
	ax = plt.subplot(111)
	for i in range(data.shape[0]):
		plt.text(data[i, 0], data[i, 1], str(label[i]), color=plt.cm.Set1(label[i] / 10),
				 fontdict={'weight': 'bold', 'size': 7})
	plt.xticks()
	plt.yticks()
	plt.title(title, fontsize=14)
	return fig


class Linear(nn.Module):
    """
    Linear Module
    """
    def __init__(self, in_dim, out_dim, bias=True, w_init='linear'):
        """
        :param in_dim: dimension of input
        :param out_dim: dimension of output
        :param bias: boolean. if True, bias is included.
        :param w_init: str. weight inits with xavier initialization.
        """
        super(Linear, self).__init__()
        self.linear_layer = nn.Linear(in_dim, out_dim, bias=bias)

        nn.init.xavier_uniform_(
            self.linear_layer.weight,
            gain=nn.init.calculate_gain(w_init))

    def forward(self, x):
        return self.linear_layer(x)



class Prefix_Encoder(torch.nn.Module):
    r'''
    The torch.nn model to encode the prefix

    Input shape: (batch-size, prefix-length)

    Output shape: (batch-size, prefix-length, 2*layers*hidden)
    '''

    def __init__(self,args):
        super().__init__()
        # Prefix mapping is used when parameters are frozen
        self.prefix_projection = args.use_prefix  ###
        # self.dropout = nn.Dropout(p=hp.enc_ffn_dropout_rate)
        if self.prefix_projection:
            self.trans = torch.nn.Sequential(
                Linear(args.pre_seq_len,args.prefix_hidden_size_1),
                nn.Dropout(p=hp.enc_ffn_dropout_rate),
                nn.ReLU(),
                Linear(args.prefix_hidden_size_1, args.prefix_hidden_size_1),
                nn.Dropout(p=hp.enc_ffn_dropout_rate),
                nn.ReLU(),
                Linear(args.prefix_hidden_size_1,args.prefix_hidden_size),
                nn.Dropout(p=hp.enc_ffn_dropout_rate),
                nn.ReLU(),
                Linear(args.prefix_hidden_size, args.pre_model_len * args.num_attn_layers * 2 * args.num_hidden),
                nn.Dropout(p=hp.enc_ffn_dropout_rate),
                nn.ReLU(),
            )

    def forward(self,prefix:torch.Tensor):
        if self.prefix_projection:
            past_key_values = self.trans(prefix)
        else:
            past_key_values = None
        return past_key_values


class MultiheadAttention_four_prefix(nn.Module):
    def __init__(self,num_hidden_k):
        """
        :param num_hidden_k: dimension of hidden
        """
        super(MultiheadAttention_four_prefix, self).__init__()
        self.num_hidden_k = num_hidden_k
        self.attn_dropout = nn.Dropout(p=hp.self_att_dropout_rate)
        self.attention = None

    def forward(self, key, value, query, mask=None, query_mask=None):
        # get attention score
        attn = torch.matmul(query,key.transpose(2,3))
        attn = attn / math.sqrt(self.num_hidden_k)

        # Masking to ignore padding (key side)
        if mask is not None:

            attn = attn.masked_fill(mask, -2 ** 32 + 1)
            # attn = attn.unsqueeze(1)
            attn = torch.softmax(attn, dim=-1)
            # attn = attn.squeeze()
        else:
            attn = torch.softmax(attn, dim=-1)

        # Dropout
        self.attention = self.attn_dropout(attn).view(key.size(0)//4, 4, key.size(1), -1, key.size(2))

        # Get Context Vector
        result = torch.matmul(attn, value)

        return result, self.attention


class Attention_four_prefix(nn.Module):
    """
    Attention Network
    """
    def __init__(self):
        """
        K: key, V: value, Q: query
        """
        super(Attention_four_prefix, self).__init__()

        self.num_hidden = hp.num_hidden
        self.num_attn_heads = hp.num_attn_heads
        self.num_hidden_per_attn = self.num_hidden // self.num_attn_heads

        self.K = Linear(self.num_hidden, self.num_hidden, bias=False)
        self.V = Linear(self.num_hidden, self.num_hidden, bias=False)
        self.Q = Linear(self.num_hidden, self.num_hidden, bias=False)

        self.multihead = MultiheadAttention_four_prefix(self.num_hidden_per_attn)

        self.residual_dropout = nn.Dropout(p=hp.self_att_block_res_dropout)
        self.final_linear = Linear(self.num_hidden * 2, self.num_hidden)
        self.layer_norm = nn.LayerNorm(self.num_hidden)

    def forward(self,memory, decoder_input, past_key_value=None,mask=None,query_mask=None):
        batch_size = memory.size(0)
        sub_rou = memory.size(1)
        seq_k = memory.size(2)
        seq_q = decoder_input.size(2)
        if past_key_value is not None:
            pre_seq = past_key_value.size(3)

        # Generate a multihead attention
        K = self.K(memory).view(batch_size, sub_rou, seq_k, self.num_attn_heads
                                , self.num_hidden_per_attn)
        V = self.V(memory).view(batch_size, sub_rou, seq_k, self.num_attn_heads
                                , self.num_hidden_per_attn)
        Q = self.Q(decoder_input).view(batch_size, sub_rou, seq_k, self.num_attn_heads
                                       , self.num_hidden_per_attn)

        K = K.permute(3, 0, 1, 2, 4).contiguous().view(-1, sub_rou, seq_k, self.num_hidden_per_attn)
        V = V.permute(3, 0, 1, 2, 4).contiguous().view(-1, sub_rou, seq_k, self.num_hidden_per_attn)
        Q = Q.permute(3, 0, 1, 2, 4).contiguous().view(-1, sub_rou, seq_k, self.num_hidden_per_attn)

        # prefix
        if past_key_value is not None:
            prefix_k = past_key_value[0].unsqueeze(2).repeat(1,1,sub_rou,1,1).view(-1,sub_rou,pre_seq,self.num_hidden_per_attn)
            prefix_v = past_key_value[1].unsqueeze(2).repeat(1,1,sub_rou,1,1).view(-1,sub_rou,pre_seq,self.num_hidden_per_attn)
            K = torch.cat([prefix_k,K],dim=2)
            V = torch.cat([prefix_v,V],dim=2)

            prefix_mask = torch.ones(batch_size*self.num_attn_heads,sub_rou,seq_q,pre_seq).to(K.device)


        if mask is not None:
            new_length = mask.size(2)
            mask = mask.unsqueeze(-1).repeat(1, 1, 1, new_length)
            mask = mask.repeat(self.num_attn_heads, 1, 1, 1)
            if past_key_value is not None:
                mask = torch.cat([prefix_mask.eq(0),mask],dim=3)

        # Get a context vector
        result, attns = self.multihead(K, V, Q, mask=mask)

        # Concatenate all multihead context vectors
        result = result.view(self.num_attn_heads, batch_size, sub_rou, seq_k, self.num_hidden_per_attn)
        result = result.permute(1, 2, 3, 0, 4).contiguous().view(batch_size, sub_rou, seq_q, -1)

        # Concatenate a context vector with a single input (most important)
        #   e.g. input (256) + result (256) -> 512
        result = torch.cat([decoder_input, result], dim=-1)

        # Final linear
        result = self.final_linear(result)

        # Residual dropout & connection
        result = self.residual_dropout(result)
        result = result + decoder_input

        # Layer normalization
        result = self.layer_norm(result)

        return result, attns


class Convolution_four(nn.Module):
    def __init__(self,in_channels,out_channels,kernel_size=1, stride=1,
                 padding=0, dilation=1,bias=True,w_init='linear'):
        super(Convolution_four, self).__init__()
        self.conv = nn.Conv1d(in_channels, out_channels,
                              kernel_size=kernel_size, stride=stride,
                              padding=padding, dilation=dilation,
                              bias=bias)
        nn.init.xavier_uniform_(
            self.conv.weight, gain=nn.init.calculate_gain(w_init))

    def forward(self, x):
        data_size = x.size()
        x = x.reshape(-1,data_size[2],data_size[3])
        x = self.conv(x)
        x = x.reshape(data_size[0],data_size[1],-1,data_size[3])
        return x


class PositionWiseFeedForward_Four(nn.Module):
    '''
    batch后新增一个通道
    '''
    def __init__(self):
        super(PositionWiseFeedForward_Four, self).__init__()
        num_hidden = hp.num_hidden
        self.w_1 = Convolution_four(num_hidden, num_hidden * 4, kernel_size=1,
                               w_init='relu')
        self.w_2 = Convolution_four(num_hidden * 4, num_hidden, kernel_size=1,
                               w_init='linear')
        self.dropout = nn.Dropout(p=hp.pos_dropout_rate)
        self.layer_norm = nn.LayerNorm(num_hidden)

    def forward(self,input_):
        # The input here is four-dimensional
        x = input_.transpose(2,3)
        x = self.w_2(torch.relu(self.w_1(x)))
        x = x.transpose(2,3)

        # dropout
        x = self.dropout(x)

        # layer normalization
        x = self.layer_norm(x)

        return x



class EncoderPrenet_four(nn.Module):
    '''
    Pre-network for Encoder consists of convolution networks.
    '''
    def __init__(self,embedding_size, channels):
        super(EncoderPrenet_four, self).__init__()
        self.embedding_size = embedding_size

        self.conv1d_layers = nn.ModuleList()
        self.bn_layers = nn.ModuleList()
        self.dropout_layers = nn.ModuleList()

        in_channels, out_channels = embedding_size, channels
        kernel_size = hp.enc_conv1d_kernel_size

        for i in range(hp.enc_conv1d_layers):
            conv1d = Convolution_four(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,
                          padding=int(np.floor(kernel_size / 2)), w_init='relu')
            self.conv1d_layers.append(conv1d)

            batch_norm = nn.BatchNorm1d(out_channels)
            self.bn_layers.append(batch_norm)

            dropout_layer = nn.Dropout(p=hp.enc_conv1d_dropout_rate)
            self.dropout_layers.append(dropout_layer)

            in_channels, out_channels = out_channels, out_channels

        self.projection = Linear(out_channels, channels)

    def forward(self, input):
        """
        :param input: Batch * Token * dimension
        :return:
        """
        input = input.transpose(2, 3)
        # data_size = input.size()
        # input = input.reshape(-1, data_size[2], data_size[3])
        for conv1d, bn, dropout in zip(self.conv1d_layers, self.bn_layers, self.dropout_layers):
            # input = dropout(torch.relu(bn(conv1d(input))))
            input = conv1d(input)
            data_size = input.size()
            input = input.reshape(-1, data_size[2], data_size[3])
            input = dropout(torch.relu(bn(input)))
            input = input.reshape(data_size)

        input = input.transpose(2, 3)
        input = self.projection(input)

        return input


class BERTEncoder_four_prefix(nn.Module):
    """
    Encoder Network
    """
    def __init__(self,args=None):
        """
        :param embedding_size: dimension of embedding
        :param num_hidden: dimension of hidden
        """
        super(BERTEncoder_four_prefix,self).__init__()
        self.num_hidden = hp.num_hidden
        self.embed_dim = hp.embed_dim
        self.enc_maxlen = hp.enc_maxlen
        self.num_attn_layers = hp.num_attn_layers
        self.num_attn_heads = hp.num_attn_heads
        # self.num_hidden = hp.num_hidden
        self.vocab_size = args.vocab_size

        # prefix
        # self.prefix_encoder = Prefix_Encoder(args)
        # self.n_embd = self.num_hidden // self.num_attn_heads
        # self.dropout = nn.Dropout(hp.prefix_dropout_rate)

        self.alpha = nn.Parameter(torch.ones(1))
        self.embed = nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=0)

        self.pos_emb = nn.Embedding.from_pretrained(
            self.get_sinusoid_encoding_table(self.enc_maxlen, self.num_hidden,
                                             padding_idx=0), freeze=True)
        self.pos_dropout = nn.Dropout(p=hp.pos_dropout_rate)
        self.encoder_prenet = EncoderPrenet_four(self.embed_dim, self.num_hidden)

        self.layers = self.clones(Attention_four_prefix(),self.num_attn_layers)
        self.pwffns = self.clones(PositionWiseFeedForward_Four(),self.num_attn_layers)



    def forward(self,x,pos,past_key_values=None):
        if self.training:
            mask = x.eq(0)
        else:
            mask = None

        x = x.type(torch.long)
        x = self.embed(x)  # B*T*d
        x = self.encoder_prenet(x)  # Three convolutions
        pos = pos.type(torch.long)
        # Get a positional embedding with an alpha and add them
        pos = self.pos_emb(pos)
        x = pos * self.alpha + x
        x = self.pos_dropout(x)

        attns = list()
        xs = list()
        i = 0
        for layer, pwffn in zip(self.layers, self.pwffns):
            if past_key_values is not None:
                x, attn = layer(x, x, past_key_value=past_key_values[i],mask=mask)
            else:
                x, attn = layer(x, x, past_key_value=None, mask=mask)
            i = i + 1
            x = pwffn(x)
            xs.append(x)
            attns.append(attn)

        return x, attns, xs

    def checkpoint(self, path, steps):
        self.save(f'{path}/mlm_checkpoint_{steps}steps.pyt')

    def save(self, path):
        torch.save(self.state_dict(), path)

    def clones(self, module, N):
        return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

    def get_sinusoid_encoding_table(self, n_position, d_hid, padding_idx=None):
        ''' Sinusoid position encoding table '''

        def cal_angle(position, hid_idx):  # 0-255
            return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)

        def get_posi_angle_vec(position):  # 0-1023
            return [cal_angle(position, hid_j) for hid_j in range(d_hid)]

        sinusoid_table = np.array(
            [get_posi_angle_vec(pos_i) for pos_i in range(n_position)])  # 1024，256  1-1024  1024

        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i
        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1

        if padding_idx is not None:
            # zero vector for padding dimension
            sinusoid_table[padding_idx] = 0.

        return torch.FloatTensor(sinusoid_table)


class BERTLanguageModel(nn.Module):
    """
    BERT Language Model
    Masked Language Model
    """
    def __init__(self, bert, vocab_size):
        """
        :param bert: BERT model which should be trained
        :param vocab_size: total vocab size for masked_lm
        """

        super().__init__()
        self.bert = bert
        self.mask_lm = MaskedLanguageModel(self.bert.num_hidden, vocab_size)
        self.init_model()

    def forward(self, x, pos):
        x, attn_list, xs = self.bert(x, pos)
        return self.mask_lm(x), attn_list

    def init_model(self):
        un_init = ['bert.embed.weight', 'bert.pos_emb.weight']
        for n, p in self.named_parameters():
            if n not in un_init and p.dim() > 1:
                nn.init.xavier_uniform_(p)

class MaskedLanguageModel(nn.Module):
    """
    predicting origin token from masked input sequence
    n-class classification problem, n-class = vocab_size
    """

    def __init__(self, hidden, vocab_size):
        """
        :param hidden: output size of BERT model
        :param vocab_size: total vocab size
        """
        super().__init__()
        self.linear = nn.Linear(hidden, vocab_size)
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x):
        return self.softmax(self.linear(x))


def warmup_cosine(x, warmup=0.002):
    if x < warmup:
        return x/warmup
    return 0.5 * (1.0 + torch.cos(math.pi * x))

def warmup_constant(x, warmup=0.002):
    if x < warmup:
        return x/warmup
    return 1.0

def warmup_linear(x, warmup=0.002):
    if x < warmup:
        return x/warmup
    return 1.0 - x


SCHEDULES = {
    'warmup_cosine':warmup_cosine,
    'warmup_constant':warmup_constant,
    'warmup_linear':warmup_linear,
}

class BertAdam(Optimizer):
    """Implements BERT version of Adam algorithm with weight decay fix.
    Params:4
        lr: learning rate
        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1
        t_total: total number of training steps for the learning
            rate schedule, -1  means constant learning rate. Default: -1
        schedule: schedule to use for the warmup (see above). Default: 'warmup_linear'
        b1: Adams b1. Default: 0.9
        b2: Adams b2. Default: 0.999
        e: Adams epsilon. Default: 1e-6
        weight_decay_rate: Weight decay. Default: 0.01
        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0
    """
    def __init__(self, params, lr, warmup=-1, t_total=-1, schedule='warmup_linear',
                 b1=hp.adam_beta1, b2=hp.adam_beta2, e=hp.epsilon,
                 weight_decay_rate=hp.adam_weight_decay_rate,
                 max_grad_norm=1.0):
        assert lr > 0.0, "Learning rate: %f - should be > 0.0" % (lr)
        assert schedule in SCHEDULES, "Invalid schedule : %s" % (schedule)
        assert 0.0 <= warmup < 1.0 or warmup == -1.0, \
            "Warmup %f - should be in 0.0 ~ 1.0 or -1 (no warm up)" % (warmup)
        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,
                        b1=b1, b2=b2, e=e, weight_decay_rate=weight_decay_rate,
                        max_grad_norm=max_grad_norm)
        super(BertAdam, self).__init__(params, defaults)

    def get_lr(self):
        """ get learning rate in training """
        lr = []
        for group in self.param_groups:
            for p in group['params']:
                state = self.state[p]
                if not state:
                    return [0]
                if group['t_total'] != -1:
                    schedule_fct = SCHEDULES[group['schedule']]
                    lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])
                else:
                    lr_scheduled = group['lr']
                lr.append(lr_scheduled)
        return lr

    def step(self, closure=None):
        """Performs a single optimization step.
        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                grad = p.grad.data
                if grad.is_sparse:
                    raise RuntimeError('Adam does not support sparse gradients, '
                                       'please consider SparseAdam instead')

                state = self.state[p]

                # State initialization
                if not state:
                    state['step'] = 0
                    # Exponential moving average of gradient values
                    state['next_m'] = torch.zeros_like(p.data)
                    # Exponential moving average of squared gradient values
                    state['next_v'] = torch.zeros_like(p.data)

                next_m, next_v = state['next_m'], state['next_v']
                beta1, beta2 = group['b1'], group['b2']

                # Add grad clipping
                if group['max_grad_norm'] > 0:
                    clip_grad_norm_(p, group['max_grad_norm'])

                # Decay the first and second moment running average coefficient
                # In-place operations to update the averages at the same time
                next_m.mul_(beta1).add_(1 - beta1, grad)
                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)
                update = next_m / (next_v.sqrt() + group['e'])

                # Just adding the square of the weights to the loss function is *not*
                # the correct way of using L2 regularization/weight decay with Adam,
                # since that will interact with the m and v parameters in strange ways.
                #
                # Instead we want to decay the weights in a manner that doesn't interact
                # with the m/v parameters. This is equivalent to adding the square
                # of the weights to the loss with plain (non-momentum) SGD.
                if group['weight_decay_rate'] > 0.0:
                    update += group['weight_decay_rate'] * p.data

                if group['t_total'] != -1:
                    schedule_fct = SCHEDULES[group['schedule']]
                    lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])
                else:
                    lr_scheduled = group['lr']

                update_with_lr = lr_scheduled * update
                p.data.add_(-update_with_lr)

                state['step'] += 1

                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1
                # No bias correction
                # bias_correction1 = 1 - beta1 ** state['step']
                # bias_correction2 = 1 - beta2 ** state['step']

        return loss

class Paths():
    def __init__(self, output_path):
        self.output_path = output_path
        self.bert_path = f'{output_path}/model_bert'
        self.mlm_path = f'{output_path}/model_mlm'
        self.prefix_path = f'{output_path}/model_freeze{hp.freeze_parameter}_prefix_{hp.pre_model_len}'
        self.fine_path = f'{output_path}/model_fine'
        self.bert_checkpoints_path = f'{output_path}/bert_checkpoints_path'
        self.contra_path = f'{output_path}/model_contra_{hp.freeze_parameter}'
        self.contra_prefixfine_path = f'{output_path}/model_contra_prefixfine_{hp.freeze_parameter}_{hp.pre_model_len}'
        self.contra_fine_path = f'{output_path}/model_contra_fine_{hp.freeze_parameter}'
        self.runs_path = f'{output_path}/runs'
        self.create_paths()

    def create_paths(self):
        os.makedirs(self.output_path, exist_ok=True)
        os.makedirs(self.bert_path, exist_ok=True)
        os.makedirs(self.mlm_path, exist_ok=True)
        os.makedirs(self.prefix_path, exist_ok=True)
        os.makedirs(self.fine_path,exist_ok=True)
        os.makedirs(self.bert_checkpoints_path, exist_ok=True)
        os.makedirs(self.contra_path, exist_ok=True)
        os.makedirs(self.contra_prefixfine_path, exist_ok=True)
        os.makedirs(self.contra_fine_path, exist_ok=True)
        os.makedirs(self.runs_path, exist_ok=True)


def optim4GPU(model, total_steps):
    """ optimizer for GPU training """
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if n not in no_decay], 'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if n in no_decay], 'weight_decay_rate': 0.0}]
    return BertAdam(optimizer_grouped_parameters,
                    lr=hp.lr,
                    warmup=hp.warmup,
                    t_total=total_steps)



class BertForSA_Prefix(nn.Module):

    def __init__(self,bert, num_labels=hp.num_label_class):
        super(BertForSA_Prefix, self).__init__()
        self.bert = bert
        self.fc = nn.Linear(hp.num_hidden, hp.num_hidden)
        self.activ = nn.Tanh()
        self.drop = nn.Dropout(0.1)
        self.classifier = nn.Linear(hp.num_hidden, num_labels)

        self.init_layer(self.classifier)

        self.pre_model_len = hp.pre_model_len
        self.num_attn_layers = hp.num_attn_layers
        self.num_attn_heads = hp.num_attn_heads
        self.prefix_encoder = Prefix_Encoder(hp)
        self.n_embd = hp.num_hidden // hp.num_attn_heads
        self.dropout = nn.Dropout(hp.prefix_dropout_rate)
        if hp.freeze_parameter:
            print("bert模型参数已冻结")
            self.freeze_parameter()
            # logger.info('111')
        else:
            print("bert模型参数未冻结")




    def get_prompt(self,prefix_token,batch_size):
        past_key_values = self.prefix_encoder(prefix_token)
        past_key_values = past_key_values.view(
            batch_size,
            self.pre_model_len,
            self.num_attn_layers * 2,
            self.num_attn_heads,
            self.n_embd
        )
        past_key_values = self.dropout(past_key_values)
        past_key_values = past_key_values.permute([2,0,3,1,4]).split(2)
        return past_key_values

    def freeze_parameter(self):
        for param in self.bert.parameters():
            param.requires_grad = False


    def forward(self, x, pos, prefix=None):
        if prefix is not None:
            a = prefix.size()[0]
            past_key_values = self.get_prompt(prefix,prefix.size()[0])
        else:
            past_key_values = None
        x, attn_list, xs = self.bert(x, pos, past_key_values)

        if hp.avg and not hp.cls:
            logits = x.mean(axis=1).mean(axis=1)
        elif hp.cls and not hp.avg:
            logits = x.mean(axis=1)[:, -1, :]
        pooled_h = self.activ(self.fc(logits))
        logits = self.classifier(self.drop(pooled_h))

        return logits

    def init_layer(self, layers):
        for p in layers.parameters():
            if p.dim() > 1: nn.init.xavier_uniform_(p)


class BertForSA_Prefix_Contrastive(nn.Module):
    def __init__(self,bert):
        super(BertForSA_Prefix_Contrastive, self).__init__()
        self.bert = bert
        # prefix
        self.pre_model_len = hp.pre_model_len
        self.num_attn_layers = hp.num_attn_layers
        self.num_attn_heads = hp.num_attn_heads
        self.prefix_encoder = Prefix_Encoder(hp)
        self.n_embd = hp.num_hidden // hp.num_attn_heads
        self.dropout = nn.Dropout(hp.prefix_dropout_rate)
        if hp.freeze_parameter:
            print("bert模型参数已冻结")
            self.freeze_parameter()
        else:
            print("bert模型参数未冻结")

    def get_prompt(self,prefix_token,batch_size):
        past_key_values = self.prefix_encoder(prefix_token)
        past_key_values = past_key_values.view(
            batch_size,
            self.pre_model_len,
            self.num_attn_layers * 2,
            self.num_attn_heads,
            self.n_embd
        )
        past_key_values = self.dropout(past_key_values)
        past_key_values = past_key_values.permute([2,0,3,1,4]).split(2)
        return past_key_values

    def freeze_parameter(self):
        for param in self.bert.parameters():
            param.requires_grad = False

    def forward(self, x, pos, prefix=None):
        if prefix is not None:
            past_key_values = self.get_prompt(prefix,prefix.size()[0])
        else:
            past_key_values = None
        x, attn_list, xs = self.bert(x, pos, past_key_values)

        if hp.avg and not hp.cls:
            logits = x.mean(axis=1).mean(axis=1)
        elif hp.cls and not hp.avg:
            logits = x.mean(axis=1)[:,-1,:]

        return logits



class BERTTrainer_prefix:
    def __init__(self, bert ,vocab_size: int,
                 train_dataloader: DataLoader, test_dataloader: DataLoader = None,
                 lr: float = hp.lr, betas=(hp.adam_beta1, hp.adam_beta2), weight_decay: float = hp.adam_weight_decay, warmup_steps=hp.warmup_steps,
                 with_cuda: bool = True, cuda_devices=None, log_freq: int = hp.log_freq, args=None, global_step=0, path=None):
        self.args = args
        self.train_step = global_step
        self.test_step = global_step
        self.path = path

        # Setup cuda device for BERT training, argument -c, --cuda should be true
        cuda_condition = torch.cuda.is_available() and with_cuda
        self.device = torch.device("cuda" if cuda_condition else "cpu")

        self.bert = bert
        if hp.pre_bert:
            # 预训练bert模型参数
            self.model = BERTLanguageModel(self.bert, vocab_size).to(self.device)
        else:
            if hp.load_model:
                print("***********load pretrained model ***********")
                self.model = torch.load(hp.load_model_path).to(self.device)
            else:
                self.model = BertForSA_Prefix(self.bert).to(self.device)

        # # Distributed GPU training if CUDA can detect more than 1 GPU
        # if with_cuda and torch.cuda.device_count() > 1:
        #     print("Using %d GPUS for BERT" % torch.cuda.device_count())
        #     self.model = nn.DataParallel(self.model, device_ids=cuda_devices)

        # Setting the train and test data loader
        self.train_data = train_dataloader
        self.test_data = test_dataloader

        total_steps = len(self.train_data) * hp.epochs
        self.optim = optim4GPU(self.model, total_steps)

        self.criterion = nn.CrossEntropyLoss()

        # Writer
        self.log_freq = log_freq
        # train
        self.train_loss_writer = SummaryWriter(f'{self.path.runs_path}/train/train_loss')
        self.train_attn_layer_writer = SummaryWriter(f'{self.path.runs_path}/train/attn_layer')

        self.num_params()
    def train(self):

        self.model.train()
        best_acc = -1
        bestloss = float('inf')
        best_epoch_loss = float('inf')
        # try:
        for epoch in range(hp.epochs):
            # self.model.train()
            # Setting the tqdm progress bar
            data_iter = tqdm.tqdm(enumerate(self.train_data),
                                  desc="EP_%s:%d" % ("train", epoch),
                                  total=len(self.train_data),
                                  bar_format="{l_bar}{r_bar}")

            running_loss = 0
            for i, data in data_iter:
                self.train_step += 1
                data["mlm_input"] = data["mlm_input"].to(self.device)
                data["input_position"] = data["input_position"].to(self.device)
                data["fine_input"] = data["fine_input"].to(self.device)
                data["fine_label"] = data["fine_label"].to(self.device)
                data["mlm_label"] = data["mlm_label"].to(self.device)
                data["prefix"] = data["prefix"].to(self.device)
                if hp.pre_bert:
                    mask_lm_output, attn_list = \
                        self.model.forward(data["mlm_input"], data["input_position"])
                    self.optim.zero_grad()
                    loss = self.criterion(mask_lm_output.transpose(2, 3).reshape(
                        data["mlm_label"].size()[0] * data["mlm_label"].size()[1], -1, data["mlm_label"].size()[2]),
                                          data["mlm_label"].reshape(
                                              data["mlm_label"].size()[0] * data["mlm_label"].size()[1], -1).long())
                else:
                    if hp.use_prefix:
                        logits = self.model.forward(data["fine_input"], data["input_position"],prefix=data["prefix"])
                    else:
                        logits = self.model.forward(data["fine_input"], data["input_position"], prefix=None)
                    self.optim.zero_grad()
                    loss = self.criterion(logits, data["fine_label"]).mean()

                loss.backward()
                self.optim.step()

                # loss
                running_loss += loss.item()
                avg_loss = running_loss / (i + 1)

                if hp.pre_bert:
                    # write a log
                    post_fix = "\tEpoch:%2d, Iter:%5d, Step:%5d, AvgLoss: %.6f, Loss: %.6f" \
                               % (epoch, i, self.train_step, avg_loss, loss.item())
                    print(post_fix)

                    if i % self.log_freq == 0:
                        data_iter.write(str(post_fix))

                    # save the bert model
                    if self.train_step % hp.save_model == 0:
                        if bestloss > avg_loss:
                            self.save_bert_model(epoch=None, step=None, best=True,
                                                 file_path=f'{self.path.bert_path}/bestloss_bert')
                            self.save_mlm_model(epoch=None, step=None, best=True,
                                                file_path=f'{self.path.mlm_path}/bestloss_mlm')
                            bestloss = avg_loss
                        self.save_bert_model(epoch=None, step=self.train_step, file_path=f"{self.path.bert_path}/bert")
                        self.save_mlm_model(epoch=None, step=self.train_step, file_path=f'{self.path.mlm_path}/mlm')
                else:
                    data_iter.set_description('epoch:%s Iter (loss=%5.8f)' % (epoch,avg_loss))

                    self.train_loss_writer.add_scalar('train_loss', loss, self.train_step)
                    if hp.use_prefix:

                        if self.train_step % hp.save_model == 0:
                            # result = self.eval()
                            total_accuracy, cm, acc, precision, recall, f1, logloss, data, label = self.eval()
                            if best_acc < total_accuracy:
                                best_acc = total_accuracy
                                self.tsne_fig(data, label)
                                self.save_bert_model(best=best_acc,file_path=f"{self.path.prefix_path}/bert_bestacc_")
                                self.save_model(best=best_acc,file_path=f"{self.path.prefix_path}/model_bestacc_")
                                self.save(self.train_step,f"{self.path.prefix_path}/",best=best_acc)
                            self.save_bert_model(epoch, file_path=f"{self.path.prefix_path}/bert_")
                            self.save_model(epoch, file_path=f"{self.path.prefix_path}/model_")
                            self.save(self.train_step, f"{self.path.prefix_path}/")

                        if hp.total_steps and hp.total_steps < self.train_step:
                            print('Epoch %d/%d : Average Loss %5.3f' % (epoch + 1, hp.epochs, running_loss / (i + 1)))
                            print('The Total Steps have been reached.')
                            self.save(self.train_step, f"{self.path.prefix_path}")
                            return
                    else:
                        if self.train_step % hp.save_model == 0:
                            total_accuracy, cm, acc, precision, recall, f1, logloss, data, label = self.eval()
                            if best_acc < total_accuracy:
                                best_acc = total_accuracy
                                self.tsne_fig(data, label)
                                self.save_bert_model(best=best_acc, file_path=f"{self.path.fine_path}/bert_bestacc_")
                                self.save_model(best=best_acc,file_path=f"{self.path.fine_path}/model_bestacc_")
                                self.save(self.train_step,  f"{self.path.fine_path}/",best=best_acc)
                            self.save_bert_model(epoch, file_path=f"{self.path.fine_path}/bert_")
                            self.save_model(epoch, file_path=f"{self.path.fine_path}/model_")
                            self.save(self.train_step, f"{self.path.fine_path}/")

                        if hp.total_steps and hp.total_steps < self.train_step:
                            print('Epoch %d/%d : Average Loss %5.3f' % (epoch + 1, hp.epochs, running_loss / (i + 1)))
                            # logger.info(
                            #     'Epoch %d/%d : Average Loss %5.3f' % (epoch + 1, hp.epochs, running_loss / (i + 1)))
                            print('The Total Steps have been reached.')
                            # logger.info('The Total Steps have been reached.')
                            self.save(self.train_step,
                                      f"{self.path.fine_path}")  # save and finish when global_steps reach total_steps
                            return
                    print('Epoch %d/%d : Average Loss %5.3f' % (epoch + 1, hp.epochs, running_loss / (i + 1)))
                    # logger.info('Epoch %d/%d : Average Loss %5.8f' % (epoch + 1, hp.epochs, running_loss / (i + 1)))
            if hp.pre_bert:
                valid_loss = self.evaluate(epoch)
                if best_epoch_loss > valid_loss:
                    best_epoch_loss = valid_loss
                    self.save_bert_model(best=True, file_path=f"{self.path.bert_path}/best_epoch_bert")
                    self.save_mlm_model(best=True, file_path=f"{self.path.mlm_path}/best_epoch_mlm")

                # Save the model after each epoch
                self.save_bert_model(epoch=epoch, file_path=f"{self.path.bert_path}/bert")
                self.save_mlm_model(epoch=epoch, file_path=f"{self.path.mlm_path}/mlm")
                # print(f"EP_{epoch}, train_avg_loss={avg_loss}, valid_avg_loss={valid_loss}")
                print(f"EP_{epoch}, train_avg_loss={avg_loss}, valid_avg_loss={valid_loss}")
            else:
                if epoch == hp.epochs - 1:
                    if not hp.debug:
                        if hp.use_prefix:

                            total_accuracy, cm, acc, precision, recall, f1, logloss, data, label = self.eval()
                            if best_acc < total_accuracy:
                                best_acc = total_accuracy
                                self.tsne_fig(data, label)
                                self.save_bert_model(best=best_acc, file_path=f"{self.path.prefix_path}/bert_bestacc_")
                                self.save_model(best=best_acc, file_path=f"{self.path.prefix_path}/model_bestacc_")
                                self.save(self.train_step, f"{self.path.prefix_path}/", best=best_acc)

                            # best_model_path = args.output_path + '/model_prefix_{}/model_best_acc.pt'.format(hp.pre_model_len)
                            # logger.info("load_best_model:{}".format(best_model_path))
                            # self.eval(best_model_path=best_model_path)
                        else:

                            total_accuracy, cm, acc, precision, recall, f1, logloss, data, label = self.eval()
                            if best_acc < total_accuracy:
                                best_acc = total_accuracy
                                self.tsne_fig(data, label)
                                self.save_bert_model(best=best_acc, file_path=f"{self.path.fine_path}/bert_bestacc_")
                                self.save_model(best=best_acc, file_path=f"{self.path.fine_path}/model_bestacc_")
                                self.save(self.train_step, f"{self.path.fine_path}/", best=best_acc)

                            # best_model_path = args.output_path + '/model_fine/model_best_acc.pt'
                            # logger.info("load_best_model:{}".format(best_model_path))
                            # self.eval(best_model_path=best_model_path)
                    else:
                        self.eval()
            # self.save_model(epoch+1, f"{self.path.bert_checkpoints_path}/")

        # print('best_acc:', best_acc)
        # self.save(self.step, f"{self.path.bert_checkpoints_path}")

    def evaluate(self, epoch):
        self.model.eval()

        # Setting the tqdm progress bar
        data_iter = tqdm.tqdm(enumerate(self.test_data),
                              desc="[+] EP_%s (%d)" % ("valid", epoch),
                              total=len(self.test_data),
                              bar_format="{l_bar}{r_bar}")
        self.test_step = 0
        running_loss = 0
        with torch.no_grad():
            for i, data in data_iter:
                self.test_step += 1

                # 0. batch_data will be sent into the device(GPU or cpu)
                data = {key: value.to(self.device) for key, value in data.items()}

                # 1. forward masked_lm model
                mask_lm_output, attn_list = \
                    self.model.forward(data["mlm_input"], data["input_position"])

                # 2. NLLLoss of predicting masked token word
                # loss = self.criterion(mask_lm_output.transpose(1, 2), data["mlm_label"])
                loss = self.criterion(
                    mask_lm_output.transpose(2, 3).reshape(data["mlm_label"].size()[0] * data["mlm_label"].size()[1],
                                                           -1, data["mlm_label"].size()[2]),
                    data["mlm_label"].reshape(data["mlm_label"].size()[0] * data["mlm_label"].size()[1], -1).long())
                # loss
                running_loss += loss.cpu().detach().numpy()
                avg_loss = running_loss / (i + 1)

                # print log
                post_fix = "\tEpoch:%2d, Iter:%5d, Step:%5d, AvgLoss: %.6f, Loss: %.6f" \
                              % (epoch, i, self.test_step, avg_loss, loss.item())

                if i % self.log_freq == 0:
                    data_iter.write(str(post_fix))

            return avg_loss

    def eval(self, best_model_path=None):

        if best_model_path is not None:
            self.model.load_state_dict(torch.load(best_model_path))
        self.model.eval()

        data_iter = tqdm.tqdm(enumerate(self.test_data),
                              total=len(self.test_data),
                              bar_format="{l_bar}{r_bar}")
        self.test_step = 0
        results = []
        accuracys = []

        predict_label_list = []
        right_label_list = []
        predict_probability_list = []
        plot_logits_list = []
        wrong_file_list = []
        with torch.no_grad():
            for i, data in data_iter:

                self.test_step += 1
                # data = {key: value.to(self.device) for key, value in data.items()}
                data["mlm_input"] = data["mlm_input"].to(self.device)
                data["fine_input"] = data["fine_input"].to(self.device)
                data["input_position"] = data["input_position"].to(self.device)
                # data["fine_label"] = one_hot(data["fine_label"], 9).to(self.device)
                data["fine_label"] = data["fine_label"].to(self.device)
                data["mlm_label"] = data["mlm_label"].to(self.device)

                data["prefix"] = data["prefix"].to(self.device)
                if hp.use_prefix:   ###
                    logits = self.model.forward(data["fine_input"], data["input_position"],prefix=data["prefix"])
                else:
                    logits = self.model.forward(data["fine_input"], data["input_position"], prefix=None)

                # logits = self.model.forward(data["mlm_input"], data["input_position"],data["prefix"])

                accuracy, result = self.calculate(logits, data["fine_label"])
                if accuracy == 0:
                    wrong_file_list.append(data["label_name"])
                results.append(result)
                accuracys.append(accuracy)

                logits_np = logits.to('cpu').numpy()
                # softmax转换
                plot_logits_list.append(logits_np)

                probs = np.exp(logits_np) / np.sum(np.exp(logits_np))
                predict_probability_list.append(probs.tolist()[0])
                predict_sample_label = self.predict_label(logits)
                predict_label_list.append(predict_sample_label.to('cpu').item() + 1)
                right_label_list.append(data["fine_label"].to('cpu').item() + 1)



        cm, acc, precision, recall, f1, logloss = self.acc_pre_recall_f1_loss(right_label_list,
                                                                              predict_label_list,
                                                                              predict_probability_list)
        data = np.vstack(plot_logits_list)
        label = np.array(right_label_list)
        total_accuracy = torch.cat(results).mean().item()
        print('results_Accuracy:%s results_rightnum:%s ', total_accuracy, sum(accuracys))
        print(f'cm:{cm}, acc:{acc}, precision:{precision}, recall:{recall}, f1:{f1}, logloss:{logloss}')
        return total_accuracy, cm, acc, precision, recall, f1, logloss,data,label
        # return total_accuracy

    def tsne_fig(self,data,label):
        print('Starting compute t-SNE Embedding...')
        ts = TSNE(n_components=2, init='pca', random_state=0)
        result = ts.fit_transform(data)
        # 调用函数，绘制图像
        fig = plot_embedding(result, label,title='')
        # 显示图像
        plt.savefig(hp.savepig_path,format="svg")

    def acc_pre_recall_f1_loss(self,y_true,y_pred,y_prob):
        # 计算混淆矩阵
        cm = confusion_matrix(y_true, y_pred)
        # print("Confusion Matrix:\n", cm)

        # 计算准确率
        acc = accuracy_score(y_true, y_pred)
        # print("Accuracy:", acc)

        # 计算精确率
        precision = precision_score(y_true, y_pred,average='macro')
        # print("Precision:", precision)

        # 计算召回率
        recall = recall_score(y_true, y_pred,average='macro')
        # print("Recall:", recall)

        # 计算F1值
        f1 = f1_score(y_true, y_pred,average='macro')
        # print("F1 score:", f1)

        # 计算log-loss

        logloss = log_loss(y_true, y_prob)
        # print("Log-Loss:", logloss)
        return cm, acc, precision, recall, f1, logloss
    def predict_label(self,logits):
        _, label_pred = logits.max(dim=1)
        return label_pred
    def calculate(self, logits, label_id):
        _, label_pred = logits.max(dim=1)
        result = (label_pred == label_id).float()  # .cpu().numpy()
        accuracy = result.mean().item()
        return accuracy, result

    def num_params(self, print_out=True):
        params_requires_grad = filter(lambda p: p.requires_grad, self.model.parameters())
        params_requires_grad = sum([np.prod(p.size()) for p in params_requires_grad]) #/ 1_000_000

        parameters = sum([np.prod(p.size()) for p in self.model.parameters()]) #/ 1_000_000
        if print_out:
            # logger.info('Trainable total Parameters: %d' % parameters)
            # logger.info('Trainable requires_grad Parameters: %d' % params_requires_grad)
            print('Trainable total Parameters: %d' % parameters)
            print('Trainable requires_grad Parameters: %d' % params_requires_grad)

    def save_model(self, epoch=None, step=None, best=None,file_path=''):
        """
        Saving the current BERT model on file_path

        :param epoch: current epoch number
        :param file_path: model output path which gonna be file_path+"ep%d" % epoch
        :return: final_output_path
        """
        if epoch is not None:
            output_path_model = file_path + "ep_%d.model" % epoch
            output_path_pt = file_path + "ep_%d.pt" % epoch
        elif step is not None:
            output_path_model = file_path + "step_%d.model" % step
            output_path_pt = file_path + "step_%d.pt" % step
        elif best is not None:
            output_path_model = file_path + ".model"
            output_path_pt = file_path + ".pt"
        torch.save(self.model.state_dict(),output_path_pt)
        torch.save(self.model.cpu(), output_path_model)
        self.model.to(self.device)
        # print("EP:%d Model Saved on:" % epoch, output_path)
        return output_path_pt,output_path_model

    def save_bert_model(self, epoch=None, step=None, best=None,file_path=''):
        """
        Saving the current BERT model on file_path
        :param epoch: current epoch number
        :param file_path: model output path which gonna be file_path+"ep%d" % epoch
        :return: final_output_path
        """
        if epoch is not None:
            output_path_model = file_path + "_ep%d.model" % epoch
            output_path_pt = file_path + "_ep%d.pt" % epoch
        elif step is not None:
            output_path_model = file_path + "_step%d.model" % step
            output_path_pt = file_path + "_step%d.pt" % step
        elif best is not None:
            output_path_model = file_path + ".model"
            output_path_pt = file_path + ".pt"
        torch.save(self.bert.state_dict(),output_path_pt)
        torch.save(self.bert.cpu(), output_path_model)
        self.bert.to(self.device)
        # print("\n[+] EP_%d BERT model (%s)" % (epoch, output_path))
        return output_path_pt,output_path_model

    def save_mlm_model(self, epoch=None, step=None, best=None,file_path=''):
        """
        Saving the current MLM model on file_path

        :param epoch: current epoch number
        :param file_path: model output path which gonna be file_path+"ep%d" % epoch
        :return: final_output_path
        """
        if epoch is not None:
            output_path_model = file_path + "ep_%d.model" % epoch
            output_path_pt = file_path + "ep_%d.pt" % epoch
        elif step is not None:
            output_path_model = file_path + "step_%d.model" % step
            output_path_pt = file_path + "step_%d.pt" % step
        elif best is not None:
            output_path_model = file_path + ".model"
            output_path_pt = file_path + ".pt"
        torch.save(self.model.state_dict(),output_path_pt)
        torch.save(self.model.cpu(), output_path_model)
        self.model.to(self.device)
        # print("EP:%d Model Saved on:" % epoch, output_path)
        return output_path_pt,output_path_model

    def save(self, i, file_path,best = None):
        """ save current model """

        if best is not None:
            torch.save(self.model.state_dict(), file_path + 'model_best_acc.pt' )
        else:
            torch.save(self.model.state_dict(),  # save model object before nn.DataParallel
                       file_path+'model_steps_' + str(i) + '.pt')