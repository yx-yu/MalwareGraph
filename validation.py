import torch
import os
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.dataset import ConcatDataset
from torch import optim
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from MCNN import MultiScaleCNN
from sppnet import SPP_NET
from Mydataset import CustomDataset, BatchSchedulerSampler
from torchvision.datasets import DatasetFolder
import torchvision
from PIL import Image
from torchvision import transforms
from utils import read_split_data, plot_data_loader_image, read_data, calculate_metrics
import os
import torch
import argparse
from torchinfo import summary
# 并行训练
# import torch.distributed as dist
import torch.nn as nn
from datetime import timedelta
import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
import numpy as np
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report

import tsne

from transformers import get_linear_schedule_with_warmup


class Trainer:
    def __init__(self,
                 model: torch.nn.Module,
                 train_dataloader: DataLoader,
                 val_data_loader: DataLoader,
                 optimizer: torch.optim.Optimizer,
                 scheduler
                 ) -> None:
        # rank
        self.model = model.to(device)
        self.train_dataloader = train_dataloader
        self.val_data_loader = val_data_loader
        self.optimizer = optimizer
        self.scheduler = scheduler

    def _run_batch(self, xs, ys, running_loss):
        output = self.model(xs)
        loss = F.cross_entropy(output, ys)
        print(
            f"Training Loss: {loss:.4f}")
        loss.backward()
        self.optimizer.step()
        self.scheduler.step()  # 对学习率进行调整(会根据预先设定的学习率调整策略，动态地调整学习率的大小，以提高训练效果)
        self.optimizer.zero_grad()
        running_loss += loss.item() * xs.size(0)
        # metrics0 = self._evaluate(num_classes=9)
        return running_loss

    def _run_epoch(self, epoch):
        running_loss = 0
        for xs, ys, padding in self.train_dataloader:
            xs = xs.to(device)
            ys = ys.to(device)
            running_loss = self._run_batch(xs, ys, running_loss)
        return running_loss

    def train(self, max_epoch: int):
        for epoch in range(max_epoch):
            # 将模型设置为评估模式
            # self.model.train()
            # running_loss = self._run_epoch(epoch)
            # epoch_loss = running_loss / len(self.train_dataloader.dataset)
            metrics = self._evaluate_img(args.num_classes)
            print(f"Epoch {epoch + 1}: {metrics}")
            # print(
            #     f"Epoch {epoch + 1} - Training Loss: {epoch_loss:.4f}")

    def _evaluate(self, num_classes):
        self.model.eval()
        # 初始化混淆矩阵和样本数
        confusion_matrix = np.zeros((num_classes, num_classes))
        num_samples = 0
        plot_logits_list = []
        plot_label_list = []
        # 遍历数据集，计算混淆矩阵和样本数
        with torch.no_grad():
            for inputs, targets, padding in self.val_data_loader:
                # 将数据移动到设备上
                inputs = inputs.to(device)
                targets = targets.to(device)

                # 前向传播，得到预测结果
                outputs = self.model(inputs)
                _, predicted = torch.max(outputs, dim=1)
                # 更新混淆矩阵和样本数
                for i in range(len(targets)):
                    confusion_matrix[targets[i], predicted[i]] += 1
                num_samples += len(targets)

                logits_np = outputs.to('cpu').numpy()
                # softmax转换
                plot_logits_list.append(logits_np)
                plot_label_list.append(targets.to('cpu').numpy())
        # 计算指标
        metrics0 = calculate_metrics(confusion_matrix)
        data = np.vstack(plot_logits_list)
        label = np.hstack(plot_label_list)
        tsne.t_sne(data, label)
        CLASS = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
        NUM_CLASS = len(CLASS)
        metrics1 = classification_report(targets.cpu(), predicted.cpu(), target_names=CLASS, digits=4,
                                         labels=list(range(NUM_CLASS)), zero_division=1)
        print(metrics0)
        # 返回结果
        return metrics0

    def _evaluate_img(self, num_classes):
        self.model.eval()
        # 初始化混淆矩阵和样本数
        confusion_matrix = np.zeros((num_classes, num_classes))
        num_samples = 0
        plot_logits_list = []
        plot_label_list = []
        # 遍历数据集，计算混淆矩阵和样本数
        with torch.no_grad():
            for inputs, targets, padding in self.val_data_loader:
                # 将数据移动到设备上
                inputs = inputs.to(device)
                targets = targets.to(device)

                # 前向传播，得到预测结果
                outputs = self.model(inputs)
                _, predicted = torch.max(outputs, dim=1)
                # 更新混淆矩阵和样本数
                for i in range(len(targets)):
                    confusion_matrix[targets[i], predicted[i]] += 1
                num_samples += len(targets)

                logits_np = outputs.to('cpu').numpy()
                # softmax转换
                plot_logits_list.append(logits_np)
                plot_label_list.append(targets.to('cpu').numpy())
        # 计算指标
        metrics0 = calculate_metrics(confusion_matrix)
        data = np.vstack(plot_logits_list)
        label = np.hstack(plot_label_list)
        tsne.t_sne(data, label)
        CLASS = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
        NUM_CLASS = len(CLASS)
        metrics1 = classification_report(targets.cpu(), predicted.cpu(), target_names=CLASS, digits=4,
                                         labels=list(range(NUM_CLASS)), zero_division=1)
        print(metrics0)
        # 返回结果
        return metrics0


# 训练函数
def train(model, dataloader, optimizer, criterion):
    model.train()
    running_loss = 0.0
    for inputs, targets, padding in dataloader:
        optimizer.zero_grad()
        # 将数据移动到指定的设备
        inputs = inputs.to(device)
        targets = targets.to(device)
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        print(
            f"Training Loss: {loss:.4f}")
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * inputs.size(0)
    epoch_loss = running_loss / len(dataloader.dataset)
    return epoch_loss


# 创建数据集和数据加载器
# 调用 BCDI/BIG/天融信数据集
root_dir = "BIG_img/width"
label_dir = "BIG_image_dimensions_label.csv"
# transforms.Normalize(normMean=[0.30137167], normStd=[0.30272897])  # train_dataset
data_transform = {
    "resize": transforms.Compose([transforms.Resize((256, 256)),
                                  transforms.ToTensor(),
                                  transforms.Normalize([0.30137167], [0.30272897])]),

    "un_resize": transforms.Compose([transforms.ToTensor(),
                                     transforms.Normalize([0.30137167], [0.30272897])])}


# data_transform = {
#     "train": transforms.Compose([transforms.RandomResizedCrop(224),
#                                  transforms.RandomHorizontalFlip(),
#                                  transforms.ToTensor(),
#                                  transforms.Normalize([0.30137167], [0.30272897])]),
#     "val": transforms.Compose([transforms.Resize(256),
#                                transforms.CenterCrop(224),
#                                transforms.ToTensor(),
#                                transforms.Normalize([0.30137167], [0.30272897])])}


def main(args):
    # # 将模型移动到指定的设备
    # # 计算global_rank和world_size
    # global_rank = local_rank + args.node_rank * args.nproc_per_node
    # world_size = args.nnode * args.nproc_per_node
    # 设置seed
    RANDOM_SEED = 1
    torch.manual_seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)

    # train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(root_dir)
    train_images_path_256, train_images_label_256, val_images_path_256, val_images_label_256 = read_data(root_dir,
                                                                                                         '256',
                                                                                                         val_rate=args.val_rate)
    train_images_path_512, train_images_label_512, val_images_path_512, val_images_label_512 = read_data(root_dir,
                                                                                                         '512',
                                                                                                         val_rate=args.val_rate)
    train_images_path_1024, train_images_label_1024, val_images_path_1024, val_images_label_1024 = read_data(root_dir,
                                                                                                             '1024',
                                                                                                             val_rate=args.val_rate)
    train_images_path_2048, train_images_label_2048, val_images_path_2048, val_images_label_2048 = read_data(root_dir,
                                                                                                             '2048',
                                                                                                             val_rate=args.val_rate)
    dataset_256 = CustomDataset(images_path=train_images_path_256,
                                images_class=train_images_label_256,
                                extensions=('.txt', '.jpg', '.png'),
                                transform=data_transform["un_resize"])
    dataset_512 = CustomDataset(images_path=train_images_path_512,
                                images_class=train_images_label_512,
                                extensions=('.txt', '.jpg', '.png'),
                                transform=data_transform["un_resize"])
    dataset_1024 = CustomDataset(images_path=train_images_path_1024,
                                 images_class=train_images_label_1024,
                                 extensions=('.txt', '.jpg', '.png'),
                                 transform=data_transform["un_resize"])
    dataset_2048 = CustomDataset(images_path=train_images_path_2048,
                                 images_class=train_images_label_2048,
                                 extensions=('.txt', '.jpg', '.png'),
                                 transform=data_transform["un_resize"])
    concat_dataset = ConcatDataset([dataset_512, dataset_1024, dataset_2048])
    # concat_dataset = ConcatDataset([dataset_512, dataset_1024, dataset_2048])

    nw = min([os.cpu_count(), args.batch_size if args.batch_size > 1 else 0, 8])  # number of workers
    print('Using {} dataloader workers'.format(nw))

    # train_dataloader = torch.utils.data.DataLoader(concat_dataset,
    #                                                batch_size=args.batch_size,
    #                                                sampler=BatchSchedulerSampler(dataset=concat_dataset,
    #                                                                              batch_size=args.batch_size),
    #                                                num_workers=4,
    #                                                shuffle=False,
    #                                                pin_memory=True,
    #                                                collate_fn=dataset_256.collate_fn)

    train_dataloader = torch.utils.data.DataLoader(concat_dataset,
                                                   batch_size=args.batch_size,
                                                   # batch input: split to each gpus (且没有任何 overlaping samples 各个 gpu 之间)
                                                   sampler=BatchSchedulerSampler(dataset=concat_dataset,
                                                                                 batch_size=args.batch_size),
                                                   num_workers=0,
                                                   shuffle=False,
                                                   pin_memory=True,
                                                   collate_fn=dataset_256.collate_fn)

    dataset_256_v = CustomDataset(images_path=val_images_path_256,
                                  images_class=val_images_label_256,
                                  extensions=('.txt', '.jpg', '.png'),
                                  transform=data_transform["un_resize"])
    dataset_512_v = CustomDataset(images_path=val_images_path_512,
                                  images_class=val_images_label_512,
                                  extensions=('.txt', '.jpg', '.png'),
                                  transform=data_transform["un_resize"])
    dataset_1024_v = CustomDataset(images_path=val_images_path_1024,
                                   images_class=val_images_label_1024,
                                   extensions=('.txt', '.jpg', '.png'),
                                   transform=data_transform["un_resize"])
    dataset_2048_v = CustomDataset(images_path=val_images_path_2048,
                                   images_class=val_images_label_2048,
                                   extensions=('.txt', '.jpg', '.png'),
                                   transform=data_transform["un_resize"])
    concat_dataset_v = ConcatDataset([dataset_256_v, dataset_512_v, dataset_1024_v, dataset_2048_v])

    nw = min([os.cpu_count(), args.batch_size if args.batch_size > 1 else 0, 8])  # number of workers

    val_data_loaderaloader = torch.utils.data.DataLoader(concat_dataset_v,
                                                         batch_size=args.batch_size,
                                                         # batch input: split to each gpus (且没有任何 overlaping samples
                                                         # 各个 gpu 之间)
                                                         num_workers=0,
                                                         shuffle=False,
                                                         pin_memory=True,
                                                         collate_fn=dataset_512_v.collate_fn)
    # 创建模型、优化器和损失函数
    model = SPP_NET(number_classes=args.num_classes)
    summary(model=model, input_size=(args.batch_size, 1, 256, 256), device="cuda")
    # 创建模型, 并将其移动到local_rank对应的GPU上
    # model = model.to(local_rank)
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, eps=1e-08)
    total_steps = len(train_dataloader) * args.max_epochs  # 训练的总步数=训练集的大小*迭代轮数
    warm_up_ratio = 0.2
    scheduler = get_linear_schedule_with_warmup(  # 定义一个线性调度器
        optimizer,
        num_warmup_steps=total_steps * warm_up_ratio,  # 学习率逐渐增加的步数，在这些步数内，学习率将从初始值逐渐增加到设定的最大值
        num_training_steps=total_steps  # 表示总的训练步数
    )
    model.load_state_dict(torch.load('model.pth'))

    trainer = Trainer(model=model, optimizer=optimizer, scheduler=scheduler, train_dataloader=train_dataloader,
                      val_data_loader=val_data_loaderaloader)
    trainer.train(args.max_epochs)
    #
    # destroy_process_group()
    # for epoch in range(10):
    #     train_loss = train(model, train_dataloader, optimizer, criterion)
    #     print(f'Epoch {epoch + 1}/{10}, Train Loss: {train_loss:.4f}')

    # 加载预训练的权重（可选）
    # model.load_state_dict(torch.load('model_weights.pth'))
    # 训练模型
    # for epoch in range(10):
    #     train_loss = train(model, train_dataloader, optimizer, loss_fn, local_rank)
    #     # test_loss, test_accuracy = do_test(model, train_dataloader, criterion)
    #     print(
    #         f"Epoch {epoch + 1} - Training Loss: {train_loss:.4f}")
    # print(
    #     f"Epoch {epoch + 1} - Training Loss: {train_loss:.4f} - Test Loss: {test_loss:.4f} - Test Accuracy: {test_accuracy:.4f}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='MCNN distributed training job')
    parser.add_argument('--seed', type=int, default=1)
    parser.add_argument('--nproc_per_node', type=int, default=1)
    parser.add_argument('--nnode', type=int, default=1)
    parser.add_argument('--node_rank', type=int, default=1)
    parser.add_argument('--batch_size', type=int, default=1, help='Input batch size on each device (default: 32)')
    parser.add_argument('--max_epochs', type=int, default=1)
    parser.add_argument('--val_rate', type=int, default=0.9)
    parser.add_argument('--num_classes', type=int, default=9)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    args = parser.parse_args()
    main(args)
