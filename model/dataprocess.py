import matplotlib.pyplot as plt
import numpy as np
import torch





def pad2d(x, max_len):
    x_pad = []
    for sub in x:
        if max_len - len(sub) < 0:
            print(len(sub))
        x_pad.append(np.pad(sub,(0, max_len - len(sub)),mode='constant'))
    return x_pad


def sub_pad(sample,max_len,max_sub):
    # Fill the seq dimension
    sample = pad2d(sample,max_len)
    template = np.array([0 for i in range(max_len)])
    # Populate the subroutine dimension
    if len(sample) < max_sub:
        temp = len(sample)
        for i in range(max_sub - temp):
            sample.append(template)

    return sample


def collate_prefix(batch, max_len):
    batch_sample_sub_len = []
    for x in batch:
        sample_sub_len = []
        for sub in x[0]:
            sample_sub_len.append(len(sub))
        batch_sample_sub_len.append(sample_sub_len)

    sub_list = [len(x[0]) for x in batch]
    max_sub = max(sub_list)
    instrs_pad = [sub_pad(x[0], max_len, max_sub) for x in batch]
    instrs = np.stack(instrs_pad)
    # mlm_input
    mlm_pad = [sub_pad(x[3], max_len, max_sub) for x in batch]
    mlm_input = np.stack(mlm_pad)

    # Convert to 0-8 labels
    fine_labels = [x[1] - 1 for x in batch]
    fine_labels = np.stack(fine_labels)

    # mlm_labels
    labels_pad = [sub_pad(x[4], max_len, max_sub) for x in batch]
    labels = np.stack(labels_pad)

    # position
    batch_position = []
    for sample_len in batch_sample_sub_len:
        sample_pos = []
        for sub_len in sample_len:
            sample_pos.append(range(1, sub_len + 1))
        batch_position.append(sample_pos)

    position = [sub_pad(x, max_len, max_sub) for x in batch_position]
    position = np.stack(position)

    # prefix
    prefix = [x[2] for x in batch]
    prefix = np.stack(prefix)

    # label_name
    label_name = x[5]

    instrs = torch.tensor(instrs).float()
    fine_labels = torch.tensor(fine_labels).long()
    mlm_input = torch.tensor(mlm_input).float()
    labels = torch.tensor(labels).float()
    position = torch.tensor(position).float()
    prefix = torch.tensor(prefix).float()

    output = {"fine_input": instrs,
              "fine_label": fine_labels,
              "mlm_input": mlm_input,
              "mlm_label": labels,
              "input_position": position,
              "prefix": prefix,
              "label_name":label_name}

    return output