# BERT language model params
embed_dim = 256
num_hidden = 128
num_attn_layers = 4
num_attn_heads = 8
enc_maxlen = 128
pos_dropout_rate = 0.1
enc_conv1d_dropout_rate = 0.2
enc_conv1d_layers = 3
enc_conv1d_kernel_size = 5
enc_ffn_dropout_rate = 0.1
self_att_dropout_rate = 0.1
self_att_block_res_dropout = 0.1

# Optimizer params
lr = 0.0005
adam_beta1 = 0.9        # [0.0-1.0]
adam_beta2 = 0.999      # [0.0-1.0]

adam_weight_decay = 0.1
adam_weight_decay_rate = 0.01


epsilon = 1e-6          # [> 0.0]
mlm_clip_grad_norm = 1.0
clip_grad_norm = True
warmup_steps = 100
warmup = 0.1

# Trainer params
epochs = 20
log_freq = 10
save_train_loss = 5000
save_valid_loss = 10000
save_model = 3000
save_checkpoint = 10000
save_runs = 10
batch_size = 1
train_dataset_ratio = 0.95

# global_step = 100000
total_steps = 2000000

with_cuda = True

# prefix
# prefix_projection = True
prefix_dropout_rate = 0.1
pre_seq_len = 256
pre_model_len = 32
prefix_hidden_size_1 = 512
prefix_hidden_size = 128


num_label_class = 5


avg = False
cls = True
#

# MLM or Fine
# train_mlm
debug = False

pre_bert = False

use_prefix = True
freeze_parameter = False

use_prebert = True


load_model = False
load_model_path = r'D:\deeplearning\malware\models\model_bestacc_.model'

contra_batchsize = 6

contra_fine_ratio = 0.1

seed = 120

# fewshot
fewshot_way = 5
fewshot_shot = 5
fewshot_query = 30
fewshot_episodes = 100

savepig_path = f'./figure/fewshot-datafountain-{fewshot_way}way-{fewshot_shot}shot-seed-ratio{contra_fine_ratio}-prefix{pre_model_len}.svg'
output_path = f'./result/private/fewshot--epoch{epochs}-seed{seed}-freeze_parameter{freeze_parameter}-contra_fine_ratio{contra_fine_ratio}-avg{avg}-{fewshot_way}way-{fewshot_shot}shot-query{fewshot_query}-episodes{fewshot_episodes}contra_batchsize{contra_batchsize}.txt'

information = globals()




