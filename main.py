import torch
import os
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.dataset import ConcatDataset
from torch import optim
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from MCNN import MultiScaleCNN
from Mydataset import CustomDataset, BatchSchedulerSampler
from torchvision.datasets import DatasetFolder
import torchvision
from PIL import Image
from torchvision import transforms
from utils import read_split_data, plot_data_loader_image, read_data, calculate_metrics
import os
import torch
import argparse
from torchinfo import summary
# 并行训练
# import torch.distributed as dist
import torch.nn as nn
from datetime import timedelta
import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
import numpy as np


def ddp_setup(rank, world_size):
    """
    Args:
        rank: Unique identifier of each process
        world_size: Total number of processes
    """
    # 配置Master Node的信息
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # 初始化Process Group
    # 关于init_method, 参数详见https://pytorch.org/docs/stable/distributed.html#initialization
    init_process_group("gloo", init_method='tcp://localhost:12355', rank=rank, world_size=world_size,
                       timeout=timedelta(seconds=5))
    torch.cuda.set_device(rank)


def cleanup():
    destroy_process_group()


class Trainer:
    def __init__(self,
                 model: torch.nn.Module,
                 train_dataloader: DataLoader,
                 val_data_loader: DataLoader,
                 optimizer: torch.optim.Optimizer,
                 gpu_id: int) -> None:
        # rank
        self.gpu_id = gpu_id
        self.model = model.to(gpu_id)
        self.train_dataloader = train_dataloader
        self.val_data_loader = val_data_loader
        self.optimizer = optimizer
        self.model = DDP(model, device_ids=[gpu_id], output_device=gpu_id)

    def _run_batch(self, xs, ys, running_loss):
        output = self.model(xs)
        loss = F.cross_entropy(output, ys)
        print(
            f"Training Loss: {loss:.4f}")
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()
        running_loss += loss.item() * xs.size(0)
        return running_loss

    def _run_epoch(self, epoch):
        batch_size = len(next(iter(self.train_dataloader))[0])
        running_loss = 0
        print(f'[GPU: {self.gpu_id}] Epoch: {epoch} | Batchsize: {batch_size} | Steps: {len(self.train_dataloader)}')
        self.train_dataloader.sampler.set_epoch(epoch)
        for xs, ys, padding in self.train_dataloader:
            xs = xs.to(self.gpu_id)
            ys = ys.to(self.gpu_id)
            running_loss = self._run_batch(xs, ys, running_loss)
        return running_loss

    def train(self, max_epoch: int):
        for epoch in range(max_epoch):
            # 将模型设置为评估模式
            self.model.train()
            running_loss = self._run_epoch(epoch)
            epoch_loss = running_loss / len(self.train_dataloader.dataset)
            metrics = self._evaluate(num_classes=9)
            print(f"Epoch {epoch + 1}: {metrics}")
            print(
                f"Epoch {epoch + 1} - Training Loss: {epoch_loss:.4f}")
            # 保存模型语句
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
            }, 'model.pth')

    def _evaluate(self, num_classes):
        """
        在给定数据集上计算模型的多分类任务指标。

        参数：
        model: 训练好的模型。
        data_loader: 包含验证数据的数据加载器。

        返回值：
        一部字典，包含5个指标的值和每个类别的假阳性率。
        """
        # 将模型设置为评估模式
        self.model.eval()

        # 初始化混淆矩阵和样本数
        confusion_matrix = np.zeros((num_classes, num_classes))
        num_samples = 0

        # 遍历数据集，计算混淆矩阵和样本数
        with torch.no_grad():
            for inputs, targets, padding in self.val_data_loader:
                # 将数据移动到设备上
                inputs = inputs.to(self.gpu_id)
                targets = targets.to(self.gpu_id)

                # 前向传播，得到预测结果
                outputs = self.model(inputs)
                _, predicted = torch.max(outputs, dim=1)
                # 更新混淆矩阵和样本数
                for i in range(len(targets)):
                    confusion_matrix[targets[i], predicted[i]] += 1
                num_samples += len(targets)
        # 计算指标
        metrics = calculate_metrics(confusion_matrix)
        # 返回结果
        return metrics


# 训练函数
def train(model, dataloader, optimizer, criterion, local_rank):
    model.train()
    running_loss = 0.0
    for inputs, targets, padding in dataloader:
        optimizer.zero_grad()
        # 将数据移动到指定的设备
        inputs = inputs.to(local_rank)
        targets = targets.to(local_rank)
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        print(
            f"Training Loss: {loss:.4f}")
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * inputs.size(0)
    epoch_loss = running_loss / len(dataloader.dataset)
    return epoch_loss



# 测试函数
def do_test(model, dataloader, criterion):
    model.eval()
    running_loss = 0.0
    correct = 0
    with torch.no_grad():
        for inputs, targets in dataloader:
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == targets).sum().item()
    epoch_loss = running_loss / len(dataloader.dataset)
    epoch_accuracy = correct / len(dataloader.dataset)
    return epoch_loss, epoch_accuracy


# 创建数据集和数据加载器
# 调用
root_dir = "Malimage/width"
label_dir = "image_dimensions_label.csv"
# transforms.Normalize(normMean=[0.30137167], normStd=[0.30272897])  # train_dataset
data_transform = {
    "train": transforms.Compose([transforms.Resize((256, 256)),
                                 transforms.ToTensor(),
                                 ]),

    "val": transforms.Compose([transforms.ToTensor(),
                               transforms.Normalize([0.30137167], [0.30272897])])}


# data_transform = {
#     "train": transforms.Compose([transforms.RandomResizedCrop(224),
#                                  transforms.RandomHorizontalFlip(),
#                                  transforms.ToTensor(),
#                                  transforms.Normalize([0.30137167], [0.30272897])]),
#     "val": transforms.Compose([transforms.Resize(256),
#                                transforms.CenterCrop(224),
#                                transforms.ToTensor(),
#                                transforms.Normalize([0.30137167], [0.30272897])])}


def main( args):
    # # 将模型移动到指定的设备
    # # 计算global_rank和world_size
    # global_rank = local_rank + args.node_rank * args.nproc_per_node
    # world_size = args.nnode * args.nproc_per_node
    # 设置seed
    torch.manual_seed(1)

    # ddp_setup(local_rank, args.world_size)

    # train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(root_dir)
    train_images_path_256, train_images_label_256, val_images_path_256, val_images_label_256 = read_data(root_dir,
                                                                                                         '256')
    train_images_path_512, train_images_label_512, val_images_path_512, val_images_label_512 = read_data(root_dir,
                                                                                                         '512')
    train_images_path_1024, train_images_label_1024, val_images_path_1024, val_images_label_1024 = read_data(root_dir,
                                                                                                             '1024')
    train_images_path_2048, train_images_label_2048, val_images_path_2048, val_images_label_2048 = read_data(root_dir,
                                                                                                             '2048')
    dataset_256 = CustomDataset(images_path=train_images_path_256,
                                images_class=train_images_label_256,
                                extensions=('.txt', '.jpg', '.png'),
                                transform=data_transform["train"])
    dataset_512 = CustomDataset(images_path=train_images_path_512,
                                images_class=train_images_label_512,
                                extensions=('.txt', '.jpg', '.png'),
                                transform=data_transform["train"])
    dataset_1024 = CustomDataset(images_path=train_images_path_1024,
                                 images_class=train_images_label_1024,
                                 extensions=('.txt', '.jpg', '.png'),
                                 transform=data_transform["train"])
    dataset_2048 = CustomDataset(images_path=train_images_path_2048,
                                 images_class=train_images_label_2048,
                                 extensions=('.txt', '.jpg', '.png'),
                                 transform=data_transform["train"])
    concat_dataset = ConcatDataset([dataset_256, dataset_512, dataset_1024, dataset_2048])

    nw = min([os.cpu_count(), args.batch_size if args.batch_size > 1 else 0, 8])  # number of workers
    print('Using {} dataloader workers'.format(nw))

    train_dataloader = torch.utils.data.DataLoader(concat_dataset,
                                                   batch_size=args.batch_size,
                                                   # batch input: split to each gpus (且没有任何 overlaping samples 各个 gpu 之间)
                                                   sampler=BatchSchedulerSampler(dataset=concat_dataset, batch_size=args.batch_size),
                                                   num_workers=4,
                                                   shuffle=False,
                                                   pin_memory=True,
                                                   collate_fn=dataset_256.collate_fn)

    dataset_256_v = CustomDataset(images_path=val_images_path_256,
                                  images_class=val_images_label_256,
                                  extensions=('.txt', '.jpg', '.png'),
                                  transform=data_transform["train"])
    dataset_512_v = CustomDataset(images_path=val_images_path_512,
                                  images_class=val_images_label_512,
                                  extensions=('.txt', '.jpg', '.png'),
                                  transform=data_transform["train"])
    dataset_1024_v = CustomDataset(images_path=val_images_path_1024,
                                   images_class=val_images_label_1024,
                                   extensions=('.txt', '.jpg', '.png'),
                                   transform=data_transform["train"])
    dataset_2048_v = CustomDataset(images_path=val_images_path_2048,
                                   images_class=val_images_label_2048,
                                   extensions=('.txt', '.jpg', '.png'),
                                   transform=data_transform["train"])
    concat_dataset_v = ConcatDataset([dataset_256_v, dataset_512_v, dataset_1024_v, dataset_2048_v])

    nw = min([os.cpu_count(), args.batch_size if args.batch_size > 1 else 0, 8])  # number of workers

    val_data_loaderaloader = torch.utils.data.DataLoader(concat_dataset_v,
                                                         batch_size=args.batch_size,
                                                         # batch input: split to each gpus (且没有任何 overlaping samples
                                                         # 各个 gpu 之间)
                                                         sampler=DistributedSampler(dataset=concat_dataset_v),
                                                         num_workers=4,
                                                         shuffle=False,
                                                         pin_memory=True,
                                                         collate_fn=dataset_256.collate_fn)

    # 创建模型、优化器和损失函数
    model = MultiScaleCNN(num_classes=9)
    summary(model=model, input_size=(args.batch_size, 1, 256, 256), device="cuda")
    # 创建模型, 并将其移动到local_rank对应的GPU上
    # model = model.to(local_rank)
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    # trainer = Trainer(model=model, gpu_id=local_rank, optimizer=optimizer, train_dataloader=train_dataloader,
    #                   val_data_loader=val_data_loaderaloader)
    # trainer.train(args.max_epochs)
    #
    # destroy_process_group()
    for epoch in range(10):
        train_loss = train(model, train_dataloader, optimizer, criterion, local_rank)
        print(f'Epoch {epoch + 1}/{10}, Train Loss: {train_loss:.4f}')


    # 加载预训练的权重（可选）
    # model.load_state_dict(torch.load('model_weights.pth'))
    # 训练模型
    # for epoch in range(10):
    #     train_loss = train(model, train_dataloader, optimizer, loss_fn, local_rank)
    #     # test_loss, test_accuracy = do_test(model, train_dataloader, criterion)
    #     print(
    #         f"Epoch {epoch + 1} - Training Loss: {train_loss:.4f}")
    # print(
    #     f"Epoch {epoch + 1} - Training Loss: {train_loss:.4f} - Test Loss: {test_loss:.4f} - Test Accuracy: {test_accuracy:.4f}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='MCNN distributed training job')
    parser.add_argument('--seed', type=int, default=1)
    parser.add_argument('--nproc_per_node', type=int, default=1)
    parser.add_argument('--nnode', type=int, default=1)
    parser.add_argument('--node_rank', type=int, default=1)
    parser.add_argument('--batch_size', type=int, default=32, help='Input batch size on each device (default: 32)')
    parser.add_argument('--max_epochs', type=int, default=10)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(device)
    if torch.cuda.is_available():
        world_size = torch.cuda.device_count()
    world_size = torch.cuda.device_count()
    print(world_size)
    parser.add_argument('--world_size', type=int, default=world_size)
    args = parser.parse_args()
    main(args)
    # mp.spawn(main, args=(args,), nprocs=world_size)
