import os
import sys
import argparse
import traceback
import copy
import math
import random
import pickle
import csv
import gc
import IPython
from itertools import combinations

import numpy as np
import pandas as pd
import torch.nn as nn
import hparams as hp
import seaborn
import matplotlib
import logging
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
matplotlib.use('Agg')
from torch.utils.data.sampler import Sampler
from scipy.spatial.distance import cdist
from contraloss import SupConLoss
import tqdm
import util
import voca
from voca import WordVocab
from contra_prefix_fewshot import *
from prefix_tuning import *

from tensorboardX import SummaryWriter


class BERTDataset_prefix(Dataset):
    def __init__(self,corpus_path,vocab,train_label_path=None,test_label_path=None,encoding='utf-8'):
        self.vocab = vocab
        self.num_data = 0
        self.corpus_path = corpus_path
        self.encoding = encoding

        with open(corpus_path, 'rb') as f:
            self.corpus = pickle.load(f)
            if hp.debug:
                self.corpus = self.corpus[1500:2000]
            self.num_data = len(self.corpus)
            # print("[+] Number of actual dataset loaded: %d" % self.num_data)
            logger.info("[+] Number of actual dataset loaded: %d" % self.num_data)
        if train_label_path is not None:
            with open(train_label_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.reader(csvfile)
                header = next(reader)  # 忽略第一行
                self.label_dict = {row[0]: int(row[1]) for row in reader}

        if test_label_path is not None:
            with open(test_label_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.reader(csvfile)
                header = next(reader)  # 忽略第一行
                self.label_dict = {row[0]: int(row[1]) for row in reader}


    def __len__(self):
        return self.num_data

    def __getitem__(self, item):
        fine_label = self.corpus[item][0]
        label_name = fine_label
        x = self.corpus[item][1]
        prefix = self.corpus[item][2]
        x, fine_label = self.string_to_num(x, fine_label, wv=self.vocab)

        x = [[self.vocab.sos_index] + x_sub + [self.vocab.eos_index] for x_sub in x]
        t = self.corpus[item][1]
        t_random, t_label = self.masking_word(t, wv=self.vocab)

        mlm_input = [[self.vocab.sos_index] + t_random_sub + [self.vocab.eos_index] for t_random_sub in t_random]
        mlm_label = [[self.vocab.sos_index] + t_label_sub + [self.vocab.eos_index] for t_label_sub in t_label]


        return x, fine_label, prefix,mlm_input,mlm_label,label_name


    def string_to_num(self,sample, label, wv=None):
        ''''''
        label = self.label_dict[label]

        ins = []
        for sub in sample:
            sub_ins = []
            for i, insn in enumerate(sub):
                if i >= hp.enc_maxlen - 3: break
                sub_ins.append(wv.voca_idx(insn))
            ins.append(sub_ins)
        return ins, label

    def masking_word(self,sample,wv=None):
        ins = []
        output_label = []
        for sub in sample:
            sub_ins = []
            output_sub_label = []
            for i, insn in enumerate(sub):
                if i >= hp.enc_maxlen-3: break
                prob = random.random()
                # randomly choose words to be replaced with a <mask>
                #    15% of the chance (original BERT paper)
                if prob < 0.15:
                    prob /= 0.15

                    # [80%] token -> mask token
                    if prob < 0.8:
                        sub_ins.append(wv.mask_index)

                    # [10%] token -> random token
                    elif prob < 0.9:
                        sub_ins.append(random.randrange(wv.vocab_size))

                    # [10%] token -> current token
                    else:
                        sub_ins.append(wv.voca_idx(insn))

                    output_sub_label.append(wv.voca_idx(insn))

                else:
                    sub_ins.append(wv.voca_idx(insn))
                    output_sub_label.append(0)
            ins.append(sub_ins)
            output_label.append(output_sub_label)
        return ins, output_label




    def forward(self, x, pos, prefix=None):
        if prefix is not None:
            # a = prefix.size()[0]
            past_key_values = self.get_prompt(prefix,prefix.size()[0])
        else:
            past_key_values = None
        x, attn_list, xs = self.bert(x, pos, past_key_values)

        if hp.avg and not hp.cls:
            # a = x.mean(axis=1)  # 这个是subroutine的均值
            # b = a.mean(axis=1)  # 这个是预训练模型的输出向量的选择方式
            # avg = a
            logits_contra = x.mean(axis=1).mean(axis=1)
        elif hp.cls and not hp.avg:
            logits_contra = x.mean(axis=1)[:,-1,:]

        pooled_h = self.activ(self.fc(logits_contra))
        logits_fine = self.classifier(self.drop(pooled_h))

        return logits_contra,logits_fine

    def init_layer(self, layers):
        for p in layers.parameters():
            if p.dim() > 1: nn.init.xavier_uniform_(p)



def train_contra_prefix_fewshot(vocab_path,args):
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    paths = Paths(args.output_path)

    wv = WordVocab.load_vocab(vocab_path)
    # print("[+] Loaded %d vocas from %s" % (wv.vocab_size, vocab_path))
    logger.info("[+] Loaded %d vocas from %s" % (wv.vocab_size, vocab_path))
    args.vocab_size = wv.vocab_size

    if args.train_dataset:
        # print("[+] Loading Train Dataset", args.train_dataset)
        logger.info("[+] Loading Train Dataset %s", args.train_dataset)

        train_dataset = BERTDataset_prefix(args.train_dataset, wv, train_label_path=args.train_label_path)
    # train_dataset, num_classes, num_support, num_query, num_episodes
    episodes = create_episodes(train_dataset,hp.fewshot_way,hp.fewshot_shot,hp.fewshot_query,hp.fewshot_episodes)
    acclist = []
    rightpredict_list = []

    for episode,selected_classes in episodes:

        support_contra_seq_sampler_epochs,support_seq_sampler,query_seq_sampler = fewshot_train_test_contra_sampler(train_dataset,episode,hp.contra_batchsize)

        # train_data_loader = DataLoader(train_dataset,sampler=support_seq_sampler,
        #                               batch_size=hp.contra_batchsize,
        #                               collate_fn=lambda batch: collate_prefix(batch, hp.enc_maxlen))
        support_dataloader = DataLoader(train_dataset,sampler=support_seq_sampler,
                                         batch_size=1,
                                         collate_fn=lambda batch: collate_prefix(batch, hp.enc_maxlen))
        test_dataloader = DataLoader(train_dataset,sampler=query_seq_sampler,
                                      batch_size=1,
                                      collate_fn=lambda batch: collate_prefix(batch, hp.enc_maxlen))
        if hp.use_prebert:
            # print("[+] load a pretrained BERT model")
            logger.info("[+] load a pretrained BERT model")
            bert = torch.load(args.model_path)
        else:
            # print("[+] Building a new BERT model")
            logger.info("[+] Building a new BERT model")
            bert = BERTEncoder_four_prefix(args)
        trainer = BERTTrainer_prefix_Contrastive_Prefix(bert, wv.vocab_size, test_dataloader=test_dataloader
                                                        , support_dataloader=support_dataloader, with_cuda=hp.with_cuda,
                                                        args=args, global_step=0, path=paths,
                                                        train_dataset=train_dataset,
                                                        support_contra_seq_sampler_epochs=support_contra_seq_sampler_epochs,
                                                        fewshot_selected_class=selected_classes)
        logger.info("[+] Start training...")
        acc,rightpredict = trainer.train_fewshot()
        acclist.append(acc)
        rightpredict_list.append(rightpredict)
    acc_mean = sum(acclist)/len(acclist)
    rightpredict_mean = sum(rightpredict_list)/len(rightpredict_list)
    logger.info("acc_mean:%s rightpredict_mean:%s" % (acc_mean,rightpredict_mean))

def train_prefix(vocab_path, args):
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    paths = Paths(args.output_path)

    wv = WordVocab.load_vocab(vocab_path)
    # print("[+] Loaded %d vocas from %s" % (wv.vocab_size, vocab_path))
    logger.info("[+] Loaded %d vocas from %s" % (wv.vocab_size, vocab_path))
    args.vocab_size = wv.vocab_size

    cuda_condition = torch.cuda.is_available()
    device = torch.device("cuda" if cuda_condition else "cpu")

    if args.train_dataset:
        # print("[+] Loading Train Dataset", args.train_dataset)
        logger.info("[+] Loading Train Dataset %s", args.train_dataset)

        train_dataset = BERTDataset_prefix(args.train_dataset,wv ,train_label_path=args.train_label_path)


        logger.info("[+] Creating train dataloaders")

        train_data_loader = DataLoader(train_dataset, batch_size=hp.batch_size,
                                       collate_fn=lambda batch: collate_prefix(batch, hp.enc_maxlen), num_workers=0,shuffle=True)

    if args.valid_dataset:
        # print("[+] Loading Valid Dataset", args.valid_dataset)
        logger.info("[+] Loading Valid Dataset %s", args.valid_dataset)
        valid_dataset = BERTDataset_prefix(args.valid_dataset,wv, test_label_path=args.test_label_path)

        valid_data_loader = DataLoader(valid_dataset, batch_size=hp.batch_size,
                                       collate_fn=lambda batch: collate_prefix(batch, hp.enc_maxlen),num_workers=0,shuffle=True)

    if hp.use_prebert:
        # print("[+] load a pretrained BERT model")
        # logger.info("[+] load a pretrained BERT model")
        # bert = BERTEncoder_four_prefix(args)
        # bert.load_state_dict(torch.load(args.model_path))
        logger.info("[+] load a contra BERT model")
        bert = torch.load(args.model_path)
    else:
        # print("[+] Building a new BERT model")
        logger.info("[+] Building a new BERT model")
        bert = BERTEncoder_four_prefix(args)

    # print("[+] Creating a BERT prefix trainer")
    logger.info("[+] Creating a BERT prefix trainer")
    trainer = BERTTrainer_prefix(bert, wv.vocab_size, train_dataloader=train_data_loader, test_dataloader=valid_data_loader,
                          with_cuda=hp.with_cuda, args=args, global_step=0, path=paths)

    # print("[+] Start training...")
    logger.info("[+] Start training...")
    trainer.train()
    # wrong#
    # trainer.eval()
    ''''''
    # trainer.eval(best_model_path='./models/pretrain_prefix32/model_prefix/bestacc_model_steps_48000.pt')


if __name__ == '__main__':
    # Make CUDA report the error when encountered it
    # os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

    parser = argparse.ArgumentParser()

    # Datasets
    parser.add_argument("-td", "--train_dataset", required=False, type=str,
                        help="train dataset for BERT training")
    parser.add_argument("-vd", "--valid_dataset", required=False, type=str,
                        help="valid dataset to evaluate a train set")
    parser.add_argument("-cd", "--corpus_dataset", required=False, type=str,
                        help="whole corpus for BERT")
    parser.add_argument("-ap", "--train_label_path",required=False, type=str,
                        help="path of label")
    parser.add_argument("-ep", "--test_label_path", required=False, type=str,
                        help="path of label")

    # Path settings
    parser.add_argument("-op", "--output_path", required=False, type=str,
                        help="path for the bert model")
    parser.add_argument("-vp", "--vocab_path", required=False, type=str,
                        help="path of vocab")
    parser.add_argument("-mp","--model_path",required=False,type=str,
                        help="path of model")

    # Others
    parser.add_argument('--seed', type=int, default=99,
                        help="random seed for initialization")
    parser.add_argument('--cuda', type=str, default='0',
                        help="cuda device")

    # prefix
    parser.add_argument('--prefix_projection',type=bool, default=False,
                        help="是否添加前缀，默认为false")

    args = parser.parse_args()

    # Prepare a set of vocabulary if needed
    vocab_path = args.vocab_path

    # Configuration logger
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)

    # Create file processor
    file_handler = logging.FileHandler(hp.output_path)
    file_handler.setLevel(logging.DEBUG)

    # Create console processor
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.DEBUG)

    # Create formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    # Add a processor to the logger
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)


    # '''prefix siyou'''

    # '''
    vocab_asm = './dataset/small'
    args.seed = hp.seed
    args.train_dataset = r'./dataset/private/private.pkl'
    args.valid_dataset = r'./dataset/private/private.pkl'

    args.output_path = './models/pretrain_prefixtuning_private'
    args.vocab_path = './corpus/pretrain.all.corpus.voca'
    args.corpus_dataset = './corpus/pretrain.all.corpus.txt'

    args.train_label_path = r'./dataset/private/train_label.csv'
    args.test_label_path = r'./dataset/private/train_label.csv'
    # args.test_label_path = './dataset/trainLabels.csv'
    args.model_path = './models/pretrain/model_bert/best_epoch_bert.model'  # best_epoch_bert.model    bestloss_bert.model
    # args.model_path = './models/pretrain/model_bert/bestloss_bert.model'
    # logger.info("[+] load a contra BERT model")
    # args.model_path = './models/bert_bestcontrapredict_.pt'
    logger.info(args)
    logger.info(hp.information)
    train_prefix(vocab_asm, args)
    # train_prefix_contrastive(vocab_asm, args)
    # '''


    '''
    vocab_asm = './dataset/small'
    args.train_dataset = r'./dataset/private/private.pkl'
    args.output_path = './models/pretrain'
    args.vocab_path = './corpus/pretrain.all.corpus.voca'
    args.train_label_path = r'./dataset/private/train_label.csv'
    args.model_path = './models/pretrain/model_bert/best_epoch_bert.model'
    logger.info(args)
    logger.info(hp.information)
    train_contra_prefix_fewshot(vocab_asm, args)
    # '''