import torch
import torch.nn as nn
import torch.nn.functional as F


# 定义空间金字塔池化操作
def spatial_pyramid_pool(x, num_levels):
    # 获取输入张量的大小
    batch, channels, height, width = x.size()

    # 定义一个列表，用于存储不同大小的池化结果
    pool_list = []

    # 对输入张量的不同区域进行不同大小的池化操作
    for i in range(num_levels):
        level = i + 1
        # 计算每个区域的大小
        pool_size = (height // level, width // level)
        # 对每个区域进行池化操作
        pool = F.avg_pool2d(x, kernel_size=pool_size, stride=pool_size)
        # 将池化结果调整为固定大小
        pool_resized = F.interpolate(pool, size=(64, 64), mode='nearest')
        # 将池化结果添加到池化结果列表中
        pool_list.append(pool_resized)

    # 将所有池化结果拼接在一起
    y = torch.cat(pool_list, dim=-1)

    return y


class MultiScaleCNN(nn.Module):
    def __init__(self, num_classes):
        super(MultiScaleCNN, self).__init__()

        # 定义第一个卷积层和池化层
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 定义第二个卷积层和池化层
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        # # 定义第三个卷积层和池化层
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 定义第四个卷积层和池化层
        self.conv4 = nn.Conv2d(384, 128, kernel_size=3, stride=2, padding=1)
        # 定义第四个卷积层和池化层
        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)
        # 定义多尺度卷积层
        self.conv4_1 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.conv4_2 = nn.Conv2d(128, 128, kernel_size=5, padding=2)
        self.conv4_3 = nn.Conv2d(128, 128, kernel_size=7, padding=3)

        # 定义全连接层和输出层
        self.fc1 = nn.Linear(64, 1)
        self.fc2 = nn.Linear(256, 128)

        self.output_layer = nn.Linear(128, num_classes)

    def forward(self, x):
        # 前向传播
        x = self.conv1(x)
        x = nn.functional.relu(x)
        x = self.pool1(x)

        x = self.conv2(x)
        x = nn.functional.relu(x)
        x = self.pool2(x)

        x = self.conv3(x)
        x = nn.functional.relu(x)
        x = self.pool3(x)

        # if x.shape[-1] == 64:
        #     x1 = self.conv4_1(x)
        #     # 将不同尺度的卷积结果拼接在一起
        #     x = x1
        # elif x.shape[-1] == 128:
        #     x1 = self.conv4_1(x)
        #     x2 = self.conv4_2(x)
        #     # 将不同尺度的卷积结果拼接在一起
        #     x = torch.cat([x1, x2], dim=1)
        # elif x.shape[-1] == 256:
        #     x1 = self.conv4_1(x)
        #     x2 = self.conv4_2(x)
        #     # 将不同尺度的卷积结果拼接在一起
        #     x = torch.cat([x1, x2], dim=1)
        # elif x.shape[-1] == 512:
        #     x1 = self.conv4_1(x)
        #     x2 = self.conv4_2(x)
        #     x3 = self.conv4_3(x)
        #     x = torch.cat([x1, x2, x3], dim=1)

        # 将不同尺度的卷积结果拼接在一起
        x1 = self.conv4_1(x)
        x2 = self.conv4_2(x)
        x3 = self.conv4_3(x)
        x = torch.cat([x1, x2, x3], dim=1)

        # 使用空间金字塔池化将 x 输出为固定大小的张量 y
        num_levels = 6  # 指定金字塔的层数
        y = spatial_pyramid_pool(x, num_levels)

        y = self.conv4(x)
        y = self.conv5(y)
        y = y.flatten(-2)
        # 将拼接结果展平
        # x = y.view(x.size(0), -1)

        x = self.fc1(y)
        # x = nn.functional.relu(x)
        x = x.transpose(1, 2).flatten(1)

        x = self.fc2(x)
        x = nn.functional.relu(x)

        x = self.output_layer(x)
        # x = nn.functional.softmax(x, dim=1)

        return x